{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711502ab-d3c4-4a34-9e58-d2d9d27f0f27",
   "metadata": {},
   "source": [
    "# Interannotator Agreement Study Report\n",
    "\n",
    "Our annotators are four participants: Zhiyang Zhou, Huiyin Lam, Vivienne, and Yushun Cheng.\n",
    "\n",
    "## Annotation Process\n",
    "\n",
    "To ensure thorough coverage and consistency checks in our annotation process, we adopted a specific task distribution strategy, detailed as follows:\n",
    "\n",
    "- Zhiyang Zhou was responsible for annotating entries 1 to 300.\n",
    "- Huiyin Lam covered entries 201 to 500.\n",
    "- Vivienne handled entries 401 to 700.\n",
    "- Yushun Cheng took on entries 601 to 900.\n",
    "\n",
    "Furthermore, the last 100 entries (801-900) were annotated collectively by all four annotators. \n",
    "\n",
    "Our annotation method ensured that every annotator had overlaps with another annotator for 100 entries, while also achieving a four-way interaction among all four annotators on the last 100 entries. The intent behind this structure is to bolster the consistency of the annotation process by ensuring entries have both pairwise overlaps and a collective four-way overlap, thereby allowing for a comprehensive analysis from multiple perspectives. This structure not only promotes detailed pairwise consistency analysis but also enriches the overall consistency analysis, ensuring the accuracy and reliability of our dataset annotations, which, in turn, secures the quality and effectiveness of subsequent data analyses.\n",
    "## Choice and Justification of Agreement Measure\n",
    "\n",
    "Given our annotation task involved multiple annotators classifying the same set of entries within a dataset, we selected **Fleiss' Kappa** as our measure of agreement to assess the overall consistency among multiple annotators in a categorical task. Additionally, for specific overlapping sections, we also calculated **Cohen's Kappa** to evaluate the pairwise consistency among annotators on particular entries. This comprehensive approach allowed us to assess agreement from both a general and detailed perspective.\n",
    "\n",
    "## Agreement Calculation and Results\n",
    "\n",
    "We began by converting our textual annotations into quantifiable numerical values, enabling manual calculation and analysis. Following this conversion and preprocessing, we calculated the Fleiss' Kappa value for the entire dataset, obtaining an approximate value of 0.9, indicative of very high agreement among annotators. The Cohen's Kappa calculations for specific overlapping areas further underscored this consistency, affirming the robustness of our annotation guidelines and training.\n",
    "\n",
    "## Discussion of Results\n",
    "\n",
    "Achieving a Fleiss' Kappa value of 0.9 signifies extremely high consistency in our annotation task, demonstrating the reliability of our annotation process. This outcome reflects the annotators' accurate understanding of the annotation guidelines and their consistency in performing the annotation task. Ensuring high consistency is crucial for any annotation project as it directly impacts the quality of data and the reliability of subsequent analyses.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The results of our agreement assessment are highly encouraging, not only showcasing the high degree of consistency among the annotation team but also proving the effectiveness of our annotation process and guidelines. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
